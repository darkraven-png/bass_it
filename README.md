
# bass_it

Minimal toolkit for bass-line analysis and generation (BPM/key detection and MIDI/bass generation).

Prerequisites

- Python 3.8+ recommended
- See `requirements.txt` for Python package dependencies

Quick install

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Run

```bash
python src/main.py
```

Project layout

- `bass.mid` â€” example MIDI file
- `requirements.txt` â€” Python dependencies
- `src/` â€” application source
  - `main.py` â€” entry point
  - `analysis/` â€” BPM and key detection modules
  - `generation/` â€” bass generation logic
  - `config/`, `constants/` â€” configuration values
- `test_audios/` â€” example audio files used for testing

Usage notes

- To reproduce results install packages from `requirements.txt` into a virtual environment.
- If you work with audio files, ensure `libsndfile` is installed on your system (often required by `python-soundfile`).
- The project currently uses `librosa` and `mido` for audio analysis and MIDI handling.

Iâ€™ll explain **how these libraries work together in a typical music/audio pipeline** and why a project might use *all of them together*.

---

# ğŸµ Typical Audio + MIDI Processing Pipeline

## 1ï¸âƒ£ Load Audio File

**Libraries involved:** `librosa`, `python-soundfile`, `numpy`

### What happens:

* `librosa` loads the audio.
* `python-soundfile` handles the low-level file reading.
* The audio becomes a `numpy` array for processing.

### Flow:

```
audio.wav â†’ soundfile â†’ librosa â†’ numpy array
```

Example:

```python
import librosa
y, sr = librosa.load("audio.wav")
```

Now:

* `y` = waveform (NumPy array)
* `sr` = sample rate

---

## 2ï¸âƒ£ Audio Feature Extraction

**Libraries involved:** `librosa`, `scipy`, `numpy`

This is where signal processing happens.

You might compute:

* Spectrogram
* MFCCs
* Tempo
* Chroma features
* Onset detection

Example:

```python
mfcc = librosa.feature.mfcc(y=y, sr=sr)
```

Under the hood:

* `librosa` uses `numpy`
* `librosa` uses `scipy.signal` for filtering & FFT

---

## 3ï¸âƒ£ Work With MIDI Files

There are **two MIDI libraries** because they serve different purposes.

---

## ğŸ¹ `mido` â†’ Low-Level MIDI Control

Best for:

* Reading raw MIDI messages
* Sending MIDI to hardware
* Editing MIDI messages manually

Example:

```python
import mido
mid = mido.MidiFile("song.mid")
for msg in mid:
    print(msg)
```

Think of `mido` as:

> â€œRaw MIDI message accessâ€

---

## ğŸ¼ `pretty_midi` â†’ High-Level Music Processing

Best for:

* Musical structure
* Notes, instruments
* Piano rolls
* Tempo analysis

Example:

```python
import pretty_midi
midi = pretty_midi.PrettyMIDI("song.mid")
piano_roll = midi.get_piano_roll()
```

Think of `pretty_midi` as:

> â€œMusic-aware MIDI manipulationâ€

---

# ğŸ¯ Why Use BOTH `mido` and `pretty_midi`?

Because they solve different problems:

| Library     | Level      | Best For                           |
| ----------- | ---------- | ---------------------------------- |
| mido        | Low-level  | MIDI message editing, hardware I/O |
| pretty_midi | High-level | Musical analysis, ML features      |

A project might:

* Use `pretty_midi` for analysis
* Use `mido` to export custom MIDI messages

---

# ğŸ§  Full Example Pipeline (Music AI Project)

Hereâ€™s how everything could connect:

```
1. Load audio â†’ librosa
2. Extract features â†’ librosa + scipy
3. Convert MIDI â†’ pretty_midi
4. Edit/export MIDI â†’ mido
5. All data stored in numpy arrays
```

---

# ğŸ¼ What Type of Projects Use This Stack?

This exact dependency list is common in:

* ğŸµ Music transcription (audio â†’ MIDI)
* ğŸ¤– AI music generation
* ğŸ¹ Piano roll modeling
* ğŸ¼ MIR (Music Information Retrieval)
* ğŸ§ Beat detection systems
* ğŸš Audio feature extraction for ML

---

# ğŸ— Simple Mental Model

Think of it like this:

* **NumPy** â†’ Math engine
* **SciPy** â†’ Advanced signal tools
* **SoundFile** â†’ Audio file reader
* **Librosa** â†’ Audio/music analysis
* **PrettyMIDI** â†’ Music structure layer
* **Mido** â†’ Raw MIDI control layer

---

If you'd like, I can also:

* Show a concrete real-world example (audio â†’ MIDI converter structure)
* Explain common errors when installing these
* Draw a simplified diagram of data flow
* Or explain how this relates to machine learning ğŸ¯


